{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EK5sz60nRQ25"
   },
   "source": [
    "# Segment Anything Model for Building Extraction From High-Resolution Satellite Imagery\n",
    "\n",
    "This notebook shows how to extract buildings within refugee/IDP settlements (or buildings in general) from high-resolution satellite imagery using the Segment Anything Model (SAM) Adapter.<br>\n",
    "\n",
    "The codes are adapted based on [SAM Adapter](https://github.com/tianrun-chen/SAM-Adapter-PyTorch) for training and [segment-geospatial](https://github.com/opengeos/segment-geospatial) for creating prediceted masks in the format of GeoTIFF and polygons in the format of ShapeFile.<br>\n",
    "\n",
    "If you use Google Colab, make sure you use GPU runtime for this notebook. Go to `Runtime` -> `Change runtime type` and select `GPU` as the hardware accelerator.For training, it is better to use A100 GPU for the sake of memory and efficiency. <br>\n",
    "\n",
    "These codes can be easily adapted for binary semantic segmentation applications in remote sensing. Feel free to use it for your own applications and implement in your local machine.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ubicubesambucket\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "sage_boto3 = boto3.client(\"sagemaker\")\n",
    "session = sagemaker.Session()\n",
    "region = session.boto_session.region_name\n",
    "bucket = \"ubicubesambucket\"\n",
    "print(bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://ubicubesambucket/temp/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "# send data to S3. SageMaker will take data from S3 in the future.\n",
    "sk_prefix = \"temp\"\n",
    "filename = 'requirements.txt'\n",
    "filepath = session.upload_data(path=filename, bucket=bucket,key_prefix=sk_prefix)\n",
    "print(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(filepath,'r')\n",
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "# Set up SageMaker session and bucket\n",
    "session = sagemaker.Session()\n",
    "bucket = 'ubicubesambucket'  # Replace with your S3 bucket name\n",
    "sk_prefix = 'temp'  # Prefix for the file in S3\n",
    "filename = 'requirements.txt'\n",
    "\n",
    "# Upload file\n",
    "filepath = session.upload_data(path=filename, bucket=bucket, key_prefix=sk_prefix)\n",
    "print(f\"File uploaded to: {filepath}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filepath' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mboto3\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Parse the file path to extract bucket and key\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m s3_path \u001b[38;5;241m=\u001b[39m \u001b[43mfilepath\u001b[49m\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms3://\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m bucket_name, key \u001b[38;5;241m=\u001b[39m s3_path\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Read the file using boto3\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'filepath' is not defined"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "# Parse the file path to extract bucket and key\n",
    "s3_path = filepath.replace(\"s3://\", \"\")\n",
    "bucket_name, key = s3_path.split('/', 1)\n",
    "\n",
    "# Read the file using boto3\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "response = s3.get_object(Bucket=bucket_name, Key=key)\n",
    "file_content = response['Body'].read().decode('utf-8')\n",
    "\n",
    "print(\"File content:\")\n",
    "print(file_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ResponseMetadata': {'RequestId': '56P96T17V3WA2G82', 'HostId': 'ZF84/vVaAFQN89HwmT8MohIawmDPKmqvAdiWLDDnaKuNVvawYEZbIT/zXSEtd8n9f/5X0lBVqgHmSvccVBmWK8PmCK4kO+8zP8DFO7fV80E=', 'HTTPStatusCode': 204, 'HTTPHeaders': {'x-amz-id-2': 'ZF84/vVaAFQN89HwmT8MohIawmDPKmqvAdiWLDDnaKuNVvawYEZbIT/zXSEtd8n9f/5X0lBVqgHmSvccVBmWK8PmCK4kO+8zP8DFO7fV80E=', 'x-amz-request-id': '56P96T17V3WA2G82', 'date': 'Sat, 04 Jan 2025 17:10:59 GMT', 'server': 'AmazonS3'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "# delete files from s3 bucket\n",
    "bucket_name = \"ubicubesambucket\"\n",
    "file_name = \"temp/requirements.txt\"\n",
    "\n",
    "s3_client = boto3.client(\"s3\")\n",
    "\n",
    "response = s3_client.delete_object(Bucket=bucket_name, Key=file_name)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ResponseMetadata': {'HTTPHeaders': {'date': 'Mon, 23 Dec 2024 19:33:38 GMT',\n",
      "                                      'server': 'AmazonS3',\n",
      "                                      'x-amz-id-2': 'gyFjC1hmNaFtYzIdzIcKjYQzFI2QFdD5oXd7SR1idjlDdp+Sw9JCF54MWDEkrgnoCy3hA9V2be4=',\n",
      "                                      'x-amz-request-id': '3AMNF0T2GR5WVW9V'},\n",
      "                      'HTTPStatusCode': 204,\n",
      "                      'HostId': 'gyFjC1hmNaFtYzIdzIcKjYQzFI2QFdD5oXd7SR1idjlDdp+Sw9JCF54MWDEkrgnoCy3hA9V2be4=',\n",
      "                      'RequestId': '3AMNF0T2GR5WVW9V',\n",
      "                      'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1468,
     "status": "ok",
     "timestamp": 1702285648780,
     "user": {
      "displayName": "Yunya Gao",
      "userId": "05443726764578958815"
     },
     "user_tz": -60
    },
    "id": "Zhw5Jxeq_y5S",
    "outputId": "a5178ba6-4eed-4aef-e08d-c7abf46073a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\YunyaGao\\Ubicube\\AWS\\SAM\\SAM_Adapter\n",
      "2.0.1+cu118\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# import pathlib\n",
    "\n",
    "# set up working directory and data folder\n",
    "# path_base = \"/content/drive/MyDrive/PhD_Research/SAM/SAM_Adapter_Final_4_Codes\"\n",
    "# os.chdir(path_base)\n",
    "path = os.getcwd() # your current working directory where your codes are stored.\n",
    "print(path)\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v9wsZ9FV7t-S"
   },
   "source": [
    "## For Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XrZrHgvda9Ub"
   },
   "source": [
    "#### Train and Inference\n",
    "\n",
    "Avaialable input prompts: <br>\n",
    "- parser = argparse.ArgumentParser()\n",
    "- parser.add_argument('--config', default=\"configs/config_sam_vit_h.yaml\", help=\"use the hyperparameters provided by SAM-Adapter\")\n",
    "- parser.add_argument('--data', default=None, help=\"different datasets\")\n",
    "- parser.add_argument('--upsample', default=\"1024\", help=\"1024 or upscaled\") \n",
    "- parser.add_argument('--size', default=\"small\", help=\"small or large\") \n",
    "- parser.add_argument('--uptype', default=\"\", help=\"cubic or SR\") \n",
    "- parser.add_argument('--epoch', default=10, help=\"epochs for training\") \n",
    "- parser.add_argument('--model_save_epoch', default=2, help=\"the interval of saving trained models, do not save models in default due to big size of model.\") \n",
    "- parser.add_argument('--inference_save_epoch', default=1, help=\"the interval of saving trained models\") \n",
    "- parser.add_argument('--thres', default=0.5, help=\"the threshold to determine the binary map\") \n",
    "\n",
    "`Change \"path_data\" in /run_sam/train.py & inference_noft.py & evaluation.py`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 88724,
     "status": "ok",
     "timestamp": 1702000658635,
     "user": {
      "displayName": "Yunya Gao",
      "userId": "05443726764578958815"
     },
     "user_tz": -60
    },
    "id": "RxKdj2TZAcpU",
    "outputId": "18ea15b8-723b-455d-a13a-122897f513f9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "[W ..\\torch\\csrc\\distributed\\c10d\\socket.cpp:601] [c10d] The client socket has failed to connect to [Yunya]:29500 (system error: 10049 - 在其上下文中，该请求的地址无效。).\n",
      "[W ..\\torch\\csrc\\distributed\\c10d\\socket.cpp:601] [c10d] The client socket has failed to connect to [Yunya]:29500 (system error: 10049 - 在其上下文中，该请求的地址无效。).\n",
      "C:\\Users\\gaoyu\\anaconda3\\envs\\samUbi\\lib\\site-packages\\mmcv\\__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.\n",
      "  warnings.warn(\n",
      "[W ..\\torch\\csrc\\distributed\\c10d\\socket.cpp:601] [c10d] The client socket has failed to connect to [Yunya]:29500 (system error: 10049 - 在其上下文中，该请求的地址无效。).\n",
      "[W ..\\torch\\csrc\\distributed\\c10d\\socket.cpp:601] [c10d] The client socket has failed to connect to [Yunya]:29500 (system error: 10049 - 在其上下文中，该请求的地址无效。).\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\YunyaGao\\Ubicube\\AWS\\SAM\\SAM_Adapter\\run_sam\\train.py\", line 31, in <module>\n",
      "    torch.distributed.init_process_group(backend='nccl') \n",
      "  File \"C:\\Users\\gaoyu\\anaconda3\\envs\\samUbi\\lib\\site-packages\\torch\\distributed\\distributed_c10d.py\", line 907, in init_process_group\n",
      "    default_pg = _new_process_group_helper(\n",
      "  File \"C:\\Users\\gaoyu\\anaconda3\\envs\\samUbi\\lib\\site-packages\\torch\\distributed\\distributed_c10d.py\", line 1013, in _new_process_group_helper\n",
      "    raise RuntimeError(\"Distributed package doesn't have NCCL \" \"built in\")\n",
      "RuntimeError: Distributed package doesn't have NCCL built in\n",
      "ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 22708) of binary: C:\\Users\\gaoyu\\anaconda3\\envs\\samUbi\\python.exe\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\gaoyu\\anaconda3\\envs\\samUbi\\lib\\runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\gaoyu\\anaconda3\\envs\\samUbi\\lib\\runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\gaoyu\\anaconda3\\envs\\samUbi\\Scripts\\torchrun.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\gaoyu\\anaconda3\\envs\\samUbi\\lib\\site-packages\\torch\\distributed\\elastic\\multiprocessing\\errors\\__init__.py\", line 346, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\gaoyu\\anaconda3\\envs\\samUbi\\lib\\site-packages\\torch\\distributed\\run.py\", line 794, in main\n",
      "    run(args)\n",
      "  File \"C:\\Users\\gaoyu\\anaconda3\\envs\\samUbi\\lib\\site-packages\\torch\\distributed\\run.py\", line 785, in run\n",
      "    elastic_launch(\n",
      "  File \"C:\\Users\\gaoyu\\anaconda3\\envs\\samUbi\\lib\\site-packages\\torch\\distributed\\launcher\\api.py\", line 134, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"C:\\Users\\gaoyu\\anaconda3\\envs\\samUbi\\lib\\site-packages\\torch\\distributed\\launcher\\api.py\", line 250, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "run_sam/train.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2025-01-04_18:45:00\n",
      "  host      : Yunya\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 22708)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# command example1\n",
    "!torchrun run_sam/train.py --data Dagaha2017 --epoch 3 --model_save_epoch 1 --inference_save_epoch 1 --size small --upsample 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/yunya/anaconda3/envs/process_data_sam/lib/python3.10/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.\n",
      "  warnings.warn(\n",
      "ipynb_checkpoints folder not found.\n",
      "config saved.\n",
      "train dataset: size=11\n",
      "  inp: shape=(3, 1024, 1024)\n",
      "  gt: shape=(1, 1024, 1024)\n",
      "model: #params=641.3M\n",
      "model_grad_params:4245556 \n",
      "model_total_params:641271604\n",
      "Predicted probability map.                                                      \n",
      "Save predicted probability map, binary map with threshold at 0.5 and shapefile.\n",
      "Predicted probability map.\n",
      "Save predicted probability map, binary map with threshold at 0.5 and shapefile.\n",
      "Predicted probability map.                                                      \n",
      "Save predicted probability map, binary map with threshold at 0.5 and shapefile.\n",
      "Predicted probability map.\n",
      "Save predicted probability map, binary map with threshold at 0.5 and shapefile.\n",
      "Predicted probability map.                                                      \n",
      "Save predicted probability map, binary map with threshold at 0.5 and shapefile.\n",
      "^C\n",
      "[2023-12-18 13:16:37,568] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGINT death signal, shutting down workers\n",
      "[2023-12-18 13:16:37,568] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2835760 closing signal SIGINT\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yunya/anaconda3/envs/SAM_Adapter/run_sam/train.py\", line 331, in <module>\n",
      "    main(config, path_output, path_model, model_save_epoch, inference_save_epoch, thres, upsample, path_test_img_list, path_test_gt_list)\n",
      "  File \"/home/yunya/anaconda3/envs/SAM_Adapter/run_sam/train.py\", line 257, in main\n",
      "    inference_main(model, config, thres, path_output_pred, upsample, path_test_img_list, path_test_gt_list)\n",
      "  File \"/home/yunya/anaconda3/envs/SAM_Adapter/run_sam/inference_ft.py\", line 247, in inference_main\n",
      "    prob_mask = inference_image(image, model)\n",
      "  File \"/home/yunya/anaconda3/envs/SAM_Adapter/run_sam/inference_ft.py\", line 151, in inference_image\n",
      "    mask_pred = mask_pred.cpu().numpy().copy()[0,0]\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# command example2\n",
    "!torchrun run_sam/train.py --data Dagaha2017 --epoch 3 --model_save_epoch 1 --size small --upsample upscaled --uptype cubic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference based on pretrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/yunya/anaconda3/envs/process_data_sam/lib/python3.10/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.\n",
      "  warnings.warn(\n",
      "config saved.\n",
      "Predicted probability map.\n",
      "Save predicted probability map, binary map with threshold at 0.5 and shapefile.\n",
      "Predicted probability map.\n",
      "Save predicted probability map, binary map with threshold at 0.5 and shapefile.\n"
     ]
    }
   ],
   "source": [
    "!torchrun run_sam/inference_ft.py --data Dagaha2017 --model_save_epoch 1 --size small --upsample upscaled --uptype cubic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5htzoTmRbSup"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "xfjRZjOo8Y57",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall: 0.0\n",
      "precision: 0.0\n",
      "f1: 0.0\n",
      "iou: 0.0\n"
     ]
    }
   ],
   "source": [
    "from evaluation import evaluation_single_main\n",
    "\n",
    "# select a specific prediction result and ground truth data to be evaluated\n",
    "# this part can be furthur automated based on needs\n",
    "path_gt = \"/home/yunya/anaconda3/envs/Data/Dagaha2017/SAM/upscaled/test/cubic/gt/dagahaley2.tif\"\n",
    "path_pred = \"/home/yunya/anaconda3/envs/SAM_Adapter/outputs/Dagaha2017/small/upscaled/cubic/epoch2/area2/pred_mask_bin0.5.tif\"\n",
    "\n",
    "evaluation_result = evaluation_single_main(path_pred, path_gt)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
