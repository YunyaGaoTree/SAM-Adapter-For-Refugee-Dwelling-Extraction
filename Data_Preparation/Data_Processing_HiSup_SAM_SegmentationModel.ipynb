{"cells":[{"cell_type":"markdown","metadata":{"id":"QF2cKXo-shGU"},"source":["# Data preparation\n","\n","- This notebook includes data preprocessing steps for [HiSup](https://github.com/SarahwXU/HiSup), [SAM-Adapter](https://github.com/tianrun-chen/SAM-Adapter-PyTorch), and [Segmentation_Models](https://github.com/qubvel/segmentation_models.pytorch).\n","- The codes mainly source from [HiSup](https://github.com/SarahwXU/HiSup).\n","- This workflow is only suitable for ***binary segmentation***. Feel free to adapt it for multiclass segmentation.\n","- You can upscale images (4 times) by a super resolution model ([EDSR](https://github.com/aswintechguy/Deep-Learning-Projects/tree/main/Super%20Resolution%20-%20OpenCV)) by OpenCV.\n","- The default structure and format of your input datasets are:<br>\n","Here we aim to convert a large geotiff image/label data into small patches for deep learning models.<br>\n","path/Data/Dataset_Name/raw/train(or test, val)/images<br>\n","path/Data/Dataset_Name/raw/train(or test, val)/images<br>\n","\n","    Dataset1<br>\n","    - raw\n","        - train\n","            - images  (geotiff, uint8, 3 bands (RGB), you can create and enhance image data in GIS software in advance)\n","            - gt      (geotiff, uint8, value:0(background), 255(targets)(not necessary to have to be 255 if it is a binary segmentation but have to be distinctive from background))\n","        - test\n","            - images\n","            - gt\n","        - val\n","            - images\n","            - gt<br>\n","    \n","    Dataset2<br>\n","        ... ...<br>"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14374,"status":"ok","timestamp":1687953553055,"user":{"displayName":"Yunya Gao","userId":"05443726764578958815"},"user_tz":-120},"id":"FbhU6onmHnIU","outputId":"bdd6692b-bbbd-4d88-f18d-cf5407119b8a"},"outputs":[{"name":"stdout","output_type":"stream","text":["/home/yunya/anaconda3/envs/Data_Preparation/Data_Preparation_Final\n"]},{"data":{"text/plain":["(['Dagaha2017', 'Djibo2019', 'Kutupalong2018', 'Minawao2017', 'Nduta2017'],\n"," ['train_large', 'train_small', 'val'])"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["# Set up paths and data types\n","\n","import os\n","from pathlib import Path\n","import glob\n","\n","path = os.getcwd() # your current working directory where your codes are stored.\n","print(path)\n","\n","from DataProcessing import data_process_hisup, data_process_sam_seg, upscale_img, upscale_lab, set_sr_model, data_process_augmentation\n","\n","# define path of data\n","path_database = \"your path/Data\" # # path of dataset\n","data_list = os.listdir(path_database)\n","data_list.sort()\n","# remove .ipynb_checkpoints if included in the folder\n","\n","# define the data types to be processed. In case you have different training datasets\n","# testing data does not have to be processed here because we would like to evaluate the model performance on a complete large testing data rather than small patches\n","type_list = ['train_large', 'train_small', 'val']\n","\n","# print all datasets to be processed, modifify the list if necessary\n","data_list, type_list"]},{"cell_type":"markdown","metadata":{"id":"kwuw2y2kSWlj"},"source":["## Data preparation for HiSup"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"6AGnBJKgshGg","outputId":"a41eb20e-502a-4eb7-fc8c-65af6da47839"},"outputs":[{"name":"stdout","output_type":"stream","text":["Start processing: Dagaha2017 train_large\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7/7 [02:14<00:00, 19.17s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Done\n","Start processing: Dagaha2017 train_small\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7/7 [00:32<00:00,  4.67s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Done\n","Start processing: Dagaha2017 val\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7/7 [00:10<00:00,  1.49s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Done\n"]}],"source":["# the default patchsize is 512.\n","\n","for dataset in data_list:\n","    for dtype in type_list:\n","\n","        path_dataset = os.path.join(path_database, dataset)\n","        print(\"Start processing: \" + dataset + \" \" + dtype)\n","\n","        data_process_hisup(path_dataset, dtype)\n","        print(\"Done\")"]},{"cell_type":"markdown","metadata":{"id":"cLuMx6CDshGh"},"source":["## Data preparation for SAM Adapter"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"60mQtLctshGh","outputId":"3ada15c5-7bbc-4d8c-8f12-a319a141544f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Start processing: Dagaha2017 train_large 1024\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7/7 [01:41<00:00, 14.44s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Done\n","Start processing: Dagaha2017 train_small 1024\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7/7 [00:14<00:00,  2.04s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Done\n","Start processing: Dagaha2017 val 1024\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7/7 [00:00<00:00,  7.90it/s]"]},{"name":"stdout","output_type":"stream","text":["Done\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# 1024 is the default patchsize for SAM adapter.\n","# change this part for your own purpose, e.g. data_list and patchsize_list\n","\n","patch_size = 1024\n","model_name = \"SAM\"\n","\n","for dataset in data_list:\n","    for dtype in type_list:\n","\n","        path_dataset = os.path.join(path_database, dataset)\n","        print(\"Start processing: \" + dataset + \" \" + dtype + \" \" + str(patch_size))\n","\n","        data_process_sam_seg(path_dataset, dtype, patch_size, model_name)\n","        print(\"Done\")"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"TWnkwa0SshGi","outputId":"0ed70e87-2f09-4b65-ba0c-b0365c4a8105"},"outputs":[{"name":"stdout","output_type":"stream","text":["Start processing: Dagaha2017 train_small 256\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7/7 [00:12<00:00,  1.85s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Done\n","Start processing: Dagaha2017 val 256\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7/7 [00:03<00:00,  1.99it/s]"]},{"name":"stdout","output_type":"stream","text":["Done\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# 256 is created for upscaling to 1024 by EDSR.\n","# change this part for your own purpose, e.g. data_list and patchsize_list\n","\n","patch_size = 256\n","model_name = \"SAM\"\n","\n","data_sr_list = ['Dagaha2017']\n","type_sr_list = ['train_small', 'val']\n","\n","for dataset in data_sr_list:\n","    for dtype in type_sr_list:\n","\n","        path_dataset = os.path.join(path_database, dataset)\n","        print(\"Start processing: \" + dataset + \" \" + dtype + \" \" + str(patch_size))\n","\n","        data_process_sam_seg(path_dataset, dtype, patch_size, model_name)\n","        print(\"Done\")"]},{"cell_type":"markdown","metadata":{"id":"8L-41yo5shGk"},"source":["## Data preparation for Segmention Model"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"CitoaK_zshGk","outputId":"416ca927-7df9-4f16-ecad-461a09b36ddb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Start processing: Dagaha2017 train_large 224\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7/7 [02:05<00:00, 17.91s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Done\n","Start processing: Dagaha2017 train_small 224\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7/7 [00:15<00:00,  2.29s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Done\n","Start processing: Dagaha2017 val 224\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7/7 [00:03<00:00,  1.94it/s]"]},{"name":"stdout","output_type":"stream","text":["Done\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# 224 is the default patchsize for segmentation model pytorch.\n","# change this part for your own purpose, e.g. data_list and patchsize_list\n","# in this research, we use the same dataset (including original data and upscaled data) for SAM, then applied random crop (cropped size is 224).\n","\n","patch_size = 224\n","model_name = \"SS\"\n","\n","for dataset in data_list:\n","    for dtype in type_list:\n","\n","        path_dataset = os.path.join(path_database, dataset)\n","        print(\"Start processing: \" + dataset + \" \" + dtype + \" \" + str(patch_size))\n","\n","        data_process_sam_seg(path_dataset, dtype, patch_size, model_name)\n","        print(\"Done\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gfyr_07RshGl"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"_XN_Zk3RshGm"},"source":["## Data preparation for Flipping and Rotation"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"luqngomDshGn","outputId":"df751b69-05a2-4cb1-aa8c-a20178d20f17"},"outputs":[{"name":"stdout","output_type":"stream","text":["horizontal_flip: images saved.\n","horizontal_flip: gt saved.\n","vertical_flip: images saved.\n","vertical_flip: gt saved.\n","rotate: images saved.\n","rotate: gt saved.\n","rotate: images saved.\n","rotate: gt saved.\n","rotate: images saved.\n","rotate: gt saved.\n"]}],"source":["dataset = \"Djibo2019\"\n","dtype = \"train_small\"\n","\n","path_base_img = os.path.join(path_database, dataset, \"SAM\", \"SR\", dtype, \"augmentation\", \"images\")\n","img_list = glob.glob(path_base_img + \"/*.png\")\n","\n","path_base_gt = os.path.join(path_database, dataset, \"SAM\", \"SR\", dtype, \"augmentation\", \"gt\")\n","gt_list = glob.glob(path_base_gt + \"/*.png\")\n","\n","operation_list = [\"horizontal_flip\", \"vertical_flip\", \"rotate\"]\n","degrees_list = [90, 180, 270]\n","op_times = 1\n","\n","for operation in operation_list:\n","\n","    if operation != \"rotate\":\n","\n","        degrees = 0 # no use here\n","        data_process_augmentation(path_base_img, img_list, path_base_gt, gt_list, dtype, operation, degrees, op_times)\n","        op_times += 1\n","\n","    elif operation == \"rotate\":\n","\n","        for degrees in degrees_list:\n","            data_process_augmentation(path_base_img, img_list, path_base_gt, gt_list, dtype, operation, degrees, op_times)\n","            op_times += 1\n","\n","    else:\n","        print(operation + \" doesn't belong to either of horizontal_flip, vertical_flip, rotate.\")\n",""]},{"cell_type":"markdown","metadata":{"id":"g-S1j71tshGp"},"source":["# Print number of patches for each type of created data"]},{"cell_type":"markdown","metadata":{"id":"Kd6XRW7QshGp"},"source":["### HiSup"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WzkLTQ89shGp","outputId":"d97967e6-d54c-404d-a924-7a12392f9cf5"},"outputs":[{"name":"stdout","output_type":"stream","text":["1260      Dagaha2017       train_large\n","224       Dagaha2017       train_small\n","63        Dagaha2017       val\n","1232      Djibo2019        train_large\n","280       Djibo2019        train_small\n","63        Djibo2019        val\n","9450      Kutupalong2018   train_large\n","2156      Kutupalong2018   train_small\n","504       Kutupalong2018   val\n","1008      Minawao2017      train_large\n","168       Minawao2017      train_small\n","28        Minawao2017      val\n","5670      Nduta2017        train_large\n","896       Nduta2017        train_small\n","294       Nduta2017        val\n"]}],"source":["model_name = \"HiSup\"\n","\n","for dataset in data_list:\n","    for dtype in type_list:\n","\n","        path_dataset = os.path.join(path_database, dataset, model_name, dtype, \"images\")\n","        alldata_list = os.listdir(path_dataset)\n","\n","        print(\"{:<7}   {:<15}  {}\".format(len(alldata_list), dataset, dtype))"]},{"cell_type":"markdown","metadata":{"id":"wJZKLFloshGq"},"source":["### SAM"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XpegAfFdshGq","outputId":"12076d2b-8b4f-435b-cf96-5ed30fb3c445"},"outputs":[{"name":"stdout","output_type":"stream","text":["350       Dagaha2017       1024    train_large\n","56        Dagaha2017       1024    train_small\n","7         Dagaha2017       1024    val\n","280       Djibo2019        1024    train_large\n","56        Djibo2019        1024    train_small\n","7         Djibo2019        1024    val\n","1848      Kutupalong2018   1024    train_large\n","420       Kutupalong2018   1024    train_small\n","112       Kutupalong2018   1024    val\n","224       Minawao2017      1024    train_large\n","56        Minawao2017      1024    train_small\n","7         Minawao2017      1024    val\n","1176      Nduta2017        1024    train_large\n","224       Nduta2017        1024    train_small\n","63        Nduta2017        1024    val\n"]}],"source":["model_name = \"SAM\"\n","patch_size = 1024\n","\n","for dataset in data_list:\n","    for dtype in type_list:\n","\n","        path_dataset = os.path.join(path_database, dataset, model_name, str(patch_size), dtype, \"images\")\n","        alldata_list = os.listdir(path_dataset)\n","\n","        print(\"{:<7}   {:<15}  {:<6}  {}\".format(len(alldata_list), dataset, patch_size, dtype))"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"MwAxGRCCshGq","outputId":"3b3b5e39-6164-4e67-f560-f8db08914746"},"outputs":[{"name":"stdout","output_type":"stream","text":["686       Dagaha2017       256     train_small\n","175       Dagaha2017       256     val\n"]}],"source":["model_name = \"SAM\"\n","patch_size = 256\n","\n","data_sr_list = ['Dagaha2017']\n","type_sr_list = ['train_small', 'val']\n","\n","for dataset in data_sr_list:\n","    for dtype in type_sr_list:\n","\n","        path_dataset = os.path.join(path_database, dataset, model_name, str(patch_size), dtype, \"images\")\n","        alldata_list = os.listdir(path_dataset)\n","\n","        print(\"{:<7}   {:<15}  {:<6}  {}\".format(len(alldata_list), dataset, patch_size, dtype))"]},{"cell_type":"markdown","metadata":{"id":"QkKKa33DshGr"},"source":["## Upscale image (optional) (256x256 to 1024x1024)\n","The upscaling section may take quite a long time."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sWu_3uapshGr"},"outputs":[],"source":["model_name = \"SAM\"\n","upscaled_folder = \"SR\"\n","\n","data_sr_list = ['Dagaha2017']\n","type_sr_list = ['train_small', 'val']\n","patch_size = 256\n","\n","for dataset in data_sr_list:\n","    for dtype in type_sr_list:\n","\n","        path_dataset = os.path.join(path_database, dataset, model_name, str(patch_size), dtype, \"images\")\n","        img_name_list = os.listdir(path_dataset)\n","\n","        for img_name in img_name_list:\n","\n","            path_ups_img = os.path.join(path_database, dataset, model_name, upscaled_folder, dtype, \"images\")\n","            Path(path_ups_img).mkdir(parents=True, exist_ok=True)\n","\n","            path_ups_lab = os.path.join(path_database, dataset, model_name, upscaled_folder, dtype, \"gt\")\n","            Path(path_ups_lab).mkdir(parents=True, exist_ok=True)\n","\n","            path_in_img = os.path.join(path_database, dataset, model_name, str(patch_size), dtype, \"images\", img_name)\n","            path_out_img = os.path.join(path_ups_img, img_name)\n","            path_in_lab = os.path.join(path_database, dataset, model_name, str(patch_size), dtype, \"gt\", img_name)\n","            path_out_lab = os.path.join(path_ups_lab, img_name)\n","\n","            # set up sr model\n","            sr_model = set_sr_model()\n","\n","            # upscale images\n","            upscale_img(path_in_img, path_out_img, sr_model)\n","\n","            # upscale labels\n","            upscale_lab(path_in_lab, path_out_lab)\n","\n","        print(\"Done: {:<15}   {:<6}  {}\".format(len(alldata_list), dataset, dtype))"]},{"cell_type":"markdown","metadata":{"id":"MzVHJn7lshGr"},"source":["## Upscale testing data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qNtDNjOZshGs"},"outputs":[],"source":["#### upscale data by nearest neighboring and bilinear interpolation\n","upscale_list = ['nearest', 'bilinear']\n","data_type = \"images\"\n","\n","for dataset in data_list:\n","    for upscaled_folder in upscale_list:\n","        upscale_testing_data_nearest_bilinear(path_database, dataset, upscaled_folder, data_type)\n","\n","\n","# upscale Ground Truth data\n","upscale_list = ['bilinear']\n","data_type = \"gt\"\n","\n","for dataset in data_list:\n","    for upscaled_folder in upscale_list:\n","        upscale_testing_data_nearest_bilinear(path_database, dataset, upscaled_folder, data_type)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LRyQ2VNlshGs"},"outputs":[],"source":["#### upscale image by SR\n","\n","upscaled_folder = \"EDSR\"\n","data_type = \"images\"\n","\n","for dataset in data_list:\n","    upscale_testing_data_SR(path_database, dataset, upscaled_folder, data_type)"]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python(hisup)","language":"python","name":"hisup"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.16"}},"nbformat":4,"nbformat_minor":0}