{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EK5sz60nRQ25"
   },
   "source": [
    "# Segment Anything Model for Refugee-Dwelling Extraction (SAM4Refugee) From High-Resolution Satellite Imagery\n",
    "\n",
    "This notebook shows how to segment refugee dwellings from high-resolution satellite imagery using the Segment Anything Model (SAM).<br>\n",
    "\n",
    "The codes are adapted based on [SAM Adapter](https://github.com/tianrun-chen/SAM-Adapter-PyTorch) for training and [segment-geospatial](https://github.com/opengeos/segment-geospatial) for creating prediceted masks in the format of GeoTIFF and polygons in the format of ShapeFile.<br>\n",
    "\n",
    "Make sure you use GPU runtime for this notebook. For Google Colab, go to `Runtime` -> `Change runtime type` and select `GPU` as the hardware accelerator.<br>\n",
    "\n",
    "For training, it is better to use A100 GPU for the sake of memory and efficiency. <br>\n",
    "\n",
    "This package can be easily adapted for binary semantic segmentation applications in remote sensing. Feel free to use it for your own applications and implement in your local machine.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['epoch13',\n",
       " 'epoch6',\n",
       " 'epoch5',\n",
       " 'epoch17',\n",
       " 'epoch19',\n",
       " 'epoch15',\n",
       " 'epoch3',\n",
       " 'epoch18',\n",
       " 'log.txt',\n",
       " 'epoch10',\n",
       " 'epoch11',\n",
       " 'epoch1',\n",
       " 'epoch2',\n",
       " 'epoch9',\n",
       " 'tensorboard',\n",
       " 'epoch16',\n",
       " 'epoch8',\n",
       " 'epoch4',\n",
       " 'epoch12',\n",
       " 'epoch14',\n",
       " 'epoch20',\n",
       " 'epoch7']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "\n",
    "path = \"./outputs_SAM/Dagaha2017/large/1024\"\n",
    "path_list = os.listdir(path)\n",
    "path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['epoch1', 'epoch2', 'epoch3', 'epoch4', 'epoch5', 'epoch6', 'epoch7', 'epoch8', 'epoch9', 'epoch10', 'epoch11', 'epoch12', 'epoch13', 'epoch14', 'epoch15', 'epoch16', 'epoch17', 'epoch18', 'epoch19', 'epoch20', 'log.txt', 'tensorboard']\n",
      "['epoch1', 'epoch2', 'epoch3', 'epoch4', 'epoch5', 'epoch6', 'epoch7', 'epoch8', 'epoch9', 'epoch10', 'epoch11', 'epoch12', 'epoch13', 'epoch14', 'epoch15', 'epoch16', 'epoch17', 'epoch18', 'epoch19', 'epoch20']\n"
     ]
    }
   ],
   "source": [
    "def sort_key(s):\n",
    "    # Extract the numerical part if the string starts with 'epoch'\n",
    "    if s.startswith('epoch'):\n",
    "        return int(re.findall(r'\\d+', s)[0])\n",
    "    # Return a high value for other strings so that they come at the end\n",
    "    return float('inf')\n",
    "\n",
    "sorted_files = sorted(path_list, key=sort_key)\n",
    "\n",
    "sorted_files_epoch = []\n",
    "\n",
    "for item in sorted_files:\n",
    "    if item.startswith(\"epoch\"):\n",
    "        sorted_files_epoch.append(item)    \n",
    "\n",
    "return sorted_files_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 1654,
     "status": "ok",
     "timestamp": 1690202510432,
     "user": {
      "displayName": "Yunya Gao",
      "userId": "05443726764578958815"
     },
     "user_tz": -120
    },
    "id": "Jki_Wrpz64IG",
    "outputId": "dbdf89ba-6b0e-4e82-9fd4-e7f06c3caab7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version\n",
      "3.11.0 (main, Mar  1 2023, 18:26:19) [GCC 11.2.0]\n",
      "Version info\n",
      "sys.version_info(major=3, minor=11, micro=0, releaselevel='final', serial=0)\n",
      "2.0.1+cu117\n",
      "Using device: cuda\n",
      "/home/yunya/anaconda3/envs/sam/SAM_Adapter\n"
     ]
    }
   ],
   "source": [
    "# print working versions\n",
    "import sys\n",
    "print(\"Python version\")\n",
    "print (sys.version)\n",
    "\n",
    "print(\"Version info\")\n",
    "print (sys.version_info)\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "path = os.getcwd()\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# datasets used here\n",
    "# 'Dagaha2017',\n",
    "# 'Djibo2019',\n",
    "# 'Kutupalong2018',\n",
    "# 'Minawao2017',\n",
    "# 'Nduta2017'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v9wsZ9FV7t-S"
   },
   "source": [
    "## For Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XrZrHgvda9Ub"
   },
   "source": [
    "#### Train and Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/yunya/anaconda3/envs/sam/lib/python3.11/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.\n",
      "  warnings.warn(\n",
      "config loaded.\n",
      "./outputs_SAM/Dagaha2017/large/1024\n",
      "train dataset: size=350\n",
      "  inp: shape=(3, 1024, 1024)\n",
      "  gt: shape=(1, 1024, 1024)\n",
      "val dataset: size=7\n",
      "  inp: shape=(3, 1024, 1024)\n",
      "  gt: shape=(1, 1024, 1024)\n",
      "model: #params=641.3M\n",
      "model_grad_params:4245556 \n",
      "model_total_params:641271604\n",
      "Predicted probability map.                                                      \n",
      "Save predicted probability map, binary map with threshold at 0.5 and shapefile.\n",
      "Get predicted binary mask with a threshold of 0.5\n",
      "Predicted probability map.                                                      \n",
      "Save predicted probability map, binary map with threshold at 0.5 and shapefile.\n",
      "Get predicted binary mask with a threshold of 0.5\n",
      "Predicted probability map.                                                      \n",
      "Save predicted probability map, binary map with threshold at 0.5 and shapefile.\n",
      "Get predicted binary mask with a threshold of 0.5\n",
      "Predicted probability map.                                                      \n",
      "Save predicted probability map, binary map with threshold at 0.5 and shapefile.\n",
      "Get predicted binary mask with a threshold of 0.5\n",
      "Predicted probability map.                                                      \n",
      "Save predicted probability map, binary map with threshold at 0.5 and shapefile.\n",
      "Get predicted binary mask with a threshold of 0.5\n",
      "train:  77%|██████████████████████████▎       | 271/350 [04:08<01:12,  1.09it/s]^C\n",
      "WARNING:torch.distributed.elastic.agent.server.api:Received 2 death signal, shutting down workers\n",
      "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 1702121 closing signal SIGINT\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yunya/anaconda3/envs/sam/SAM_Adapter/run_sam/train.py\", line 328, in <module>\n",
      "    main(config, path_output, path_model, model_save_epoch, inference_save_epoch, thres)\n",
      "  File \"/home/yunya/anaconda3/envs/sam/SAM_Adapter/run_sam/train.py\", line 196, in main\n",
      "    train_loss_G = train(train_loader, model)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/yunya/anaconda3/envs/sam/SAM_Adapter/run_sam/train.py\", line 131, in train\n",
      "    model.optimize_parameters()\n",
      "  File \"/home/yunya/anaconda3/envs/sam/SAM_Adapter/models/sam.py\", line 257, in optimize_parameters\n",
      "    self.forward()\n",
      "  File \"/home/yunya/anaconda3/envs/sam/SAM_Adapter/models/sam.py\", line 178, in forward\n",
      "    self.features = self.image_encoder(self.input)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/yunya/anaconda3/envs/sam/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/yunya/anaconda3/envs/sam/SAM_Adapter/models/mmseg/models/sam/image_encoder.py\", line 146, in forward\n",
      "    x = blk(x)\n",
      "        ^^^^^^\n",
      "  File \"/home/yunya/anaconda3/envs/sam/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/yunya/anaconda3/envs/sam/SAM_Adapter/models/mmseg/models/sam/image_encoder.py\", line 423, in forward\n",
      "    x = self.attn(x)\n",
      "        ^^^^^^^^^^^^\n",
      "  File \"/home/yunya/anaconda3/envs/sam/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/yunya/anaconda3/envs/sam/SAM_Adapter/models/mmseg/models/sam/image_encoder.py\", line 483, in forward\n",
      "    attn = add_decomposed_rel_pos(attn, q, self.rel_pos_h, self.rel_pos_w, (H, W), (H, W))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/yunya/anaconda3/envs/sam/SAM_Adapter/models/mmseg/models/sam/image_encoder.py\", line 599, in add_decomposed_rel_pos\n",
      "    Rw = get_rel_pos(q_w, k_w, rel_pos_w)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/yunya/anaconda3/envs/sam/SAM_Adapter/models/mmseg/models/sam/image_encoder.py\", line 541, in get_rel_pos\n",
      "    def get_rel_pos(q_size: int, k_size: int, rel_pos: torch.Tensor) -> torch.Tensor:\n",
      "    \n",
      "KeyboardInterrupt\n",
      "/home/yunya/anaconda3/envs/sam/lib/python3.11/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "!torchrun run_sam/train.py --data Dagaha2017 --size large --upsample 1024 \n",
    "!torchrun run_sam/train.py --data Dagaha2017 --size small --upsample 1024 \n",
    "!torchrun run_sam/train.py --data Dagaha2017 --size small --upsample SR --uptype nearest\n",
    "!torchrun run_sam/train.py --data Dagaha2017 --size small --upsample SR --uptype bilinear\n",
    "!torchrun run_sam/train.py --data Dagaha2017 --size small --upsample SR --uptype EDSR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!torchrun run_sam/train.py --data Djibo2019 --size large --upsample 1024 \n",
    "!torchrun run_sam/train.py --data Djibo2019 --size small --upsample 1024 \n",
    "!torchrun run_sam/train.py --data Djibo2019 --size small --upsample SR --uptype nearest\n",
    "!torchrun run_sam/train.py --data Djibo2019 --size small --upsample SR --uptype bilinear\n",
    "!torchrun run_sam/train.py --data Djibo2019 --size small --upsample SR --uptype EDSR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!torchrun run_sam/train.py --data Minawao2017 --size large --upsample 1024 \n",
    "!torchrun run_sam/train.py --data Minawao2017 --size small --upsample 1024 \n",
    "!torchrun run_sam/train.py --data Minawao2017 --size small --upsample SR --uptype nearest\n",
    "!torchrun run_sam/train.py --data Minawao2017 --size small --upsample SR --uptype bilinear\n",
    "!torchrun run_sam/train.py --data Minawao2017 --size small --upsample SR --uptype EDSR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!torchrun run_sam/train.py --data Nduta2017 --size large --upsample 1024 \n",
    "!torchrun run_sam/train.py --data Nduta2017 --size small --upsample 1024 \n",
    "\n",
    "!torchrun run_sam/train.py --data Kutupalong2018 --size large --upsample 1024 \n",
    "!torchrun run_sam/train.py --data Kutupalong2018 --size small --upsample 1024 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!torchrun run_sam/inference_noft.py --data Dagaha2017 --upsample 1024 \n",
    "!torchrun run_sam/inference_noft.py --data Dagaha2017 --upsample SR --uptype nearest\n",
    "!torchrun run_sam/inference_noft.py --data Dagaha2017 --upsample SR --uptype bilinear\n",
    "!torchrun run_sam/inference_noft.py --data Dagaha2017 --upsample SR --uptype EDSR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!torchrun run_sam/inference_noft.py --data Djibo2019 --upsample 1024 \n",
    "!torchrun run_sam/inference_noft.py --data Djibo2019 --upsample SR --uptype nearest\n",
    "!torchrun run_sam/inference_noft.py --data Djibo2019 --upsample SR --uptype bilinear\n",
    "!torchrun run_sam/inference_noft.py --data Djibo2019 --upsample SR --uptype EDSR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./outputs_SAM/Minawao2017/noFT/1024\n",
      "100%|███████████████████████████████████████████| 28/28 [00:58<00:00,  2.09s/it]\n",
      "./outputs_SAM/Minawao2017/noFT/SR/nearest\n",
      " 27%|██████████▉                              | 100/375 [02:59<07:52,  1.72s/it]"
     ]
    }
   ],
   "source": [
    "!torchrun run_sam/inference_noft.py --data Minawao2017 --upsample 1024 \n",
    "!torchrun run_sam/inference_noft.py --data Minawao2017 --upsample SR --uptype nearest\n",
    "!torchrun run_sam/inference_noft.py --data Minawao2017 --upsample SR --uptype bilinear\n",
    "!torchrun run_sam/inference_noft.py --data Minawao2017 --upsample SR --uptype EDSR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!torchrun run_sam/inference_noft.py --data Nduta2017 --upsample 1024 \n",
    "!torchrun run_sam/inference_noft.py --data Nduta2017 --upsample 1024 \n",
    "!torchrun run_sam/inference_noft.py --data Nduta2017 --upsample SR --uptype EDSR\n",
    "\n",
    "!torchrun run_sam/inference_noft.py --data Kutupalong2018 --upsample 1024 \n",
    "!torchrun run_sam/inference_noft.py --data Kutupalong2018 --upsample 1024 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5htzoTmRbSup"
   },
   "source": [
    "#### Evaluation of testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "sam1",
   "language": "python",
   "name": "sam1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
